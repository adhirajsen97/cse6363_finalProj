{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stage 5 – Baseline Ridge Regression\n",
        "\n",
        "**Goal:** Train a simple, regularized linear model on historical seasons and quantify how much error grows on newer eras.\n",
        "\n",
        "**Plan:**\n",
        "1. Load and clean the player-season dataset built earlier.\n",
        "2. Split seasons chronologically so training stops at 2021, validation is 2022, and tests are 2023–2024.\n",
        "3. Fit a Ridge regression baseline with time-aware cross-validation to choose the regularization strength.\n",
        "4. Report MAE, RMSE, and R² for train, 2022 validation, and the two post-2022 seasons.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from src.data_prep import clean_data, load_data, train_val_test_split\n",
        "from src.models import train_ridge\n",
        "from src.features import build_feature_matrix\n",
        "from src.evaluation import evaluate_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 6,054 rows from ../data/raw/player_season_2015_2024.csv\n",
            "Remaining after cleaning: 4,207\n",
            "Train rows: 2880\n",
            "Validation rows: 457\n",
            "Test 2023 rows: 436\n",
            "Test 2024 rows: 434\n",
            "Train seasons 2015–2021, validation [2022], tests [2023, 2024]\n"
          ]
        }
      ],
      "source": [
        "DATA_PATH = Path(\"../data/raw/player_season_2015_2024.csv\")\n",
        "\n",
        "if not DATA_PATH.exists():\n",
        "    raise FileNotFoundError(\n",
        "        \"Expected the canonical player-season file at ../data/raw/player_season_2015_2024.csv. \"\n",
        "        \"Run python -m src.build_player_season_dataset first.\"\n",
        "    )\n",
        "\n",
        "raw_df = load_data(DATA_PATH)\n",
        "print(f\"Loaded {len(raw_df):,} rows from {DATA_PATH}\")\n",
        "\n",
        "df = clean_data(raw_df)\n",
        "print(f\"Remaining after cleaning: {len(df):,}\")\n",
        "\n",
        "train_df, val_df, test_2023_df, test_2024_df = train_val_test_split(df)\n",
        "print(\n",
        "    f\"Train seasons {train_df['season'].min()}–{train_df['season'].max()}, \"\n",
        "    f\"validation {val_df['season'].unique().tolist()}, \"\n",
        "    f\"tests {[2023, 2024]}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaning target and position columns...\n",
            "Train: 2880 rows before cleaning\n",
            "Val: 457 rows before cleaning\n",
            "Train: 2880 rows after cleaning (dropped 0)\n",
            "Val: 457 rows after cleaning (dropped 0)\n",
            "✓ All target and position columns are clean (no NaN, None, or string 'NaN' values)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/adhirajsen/Documents/Documents - Adhiraj’s MacBook Pro/UTA/Semesters/2. Fall2025/CSE6363/FinalProject/cse6363_finalProj/.venv/lib/python3.13/site-packages/sklearn/utils/extmath.py:1144: RuntimeWarning: invalid value encountered in divide\n",
            "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
            "/Users/adhirajsen/Documents/Documents - Adhiraj’s MacBook Pro/UTA/Semesters/2. Fall2025/CSE6363/FinalProject/cse6363_finalProj/.venv/lib/python3.13/site-packages/sklearn/utils/extmath.py:1149: RuntimeWarning: invalid value encountered in divide\n",
            "  T = new_sum / new_sample_count\n",
            "/Users/adhirajsen/Documents/Documents - Adhiraj’s MacBook Pro/UTA/Semesters/2. Fall2025/CSE6363/FinalProject/cse6363_finalProj/.venv/lib/python3.13/site-packages/sklearn/utils/extmath.py:1169: RuntimeWarning: invalid value encountered in divide\n",
            "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "\nAll the 25 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n25 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/adhirajsen/Documents/Documents - Adhiraj’s MacBook Pro/UTA/Semesters/2. Fall2025/CSE6363/FinalProject/cse6363_finalProj/.venv/lib/python3.13/site-packages/sklearn/model_selection/_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/adhirajsen/Documents/Documents - Adhiraj’s MacBook Pro/UTA/Semesters/2. Fall2025/CSE6363/FinalProject/cse6363_finalProj/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 1365, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/Users/adhirajsen/Documents/Documents - Adhiraj’s MacBook Pro/UTA/Semesters/2. Fall2025/CSE6363/FinalProject/cse6363_finalProj/.venv/lib/python3.13/site-packages/sklearn/linear_model/_ridge.py\", line 1238, in fit\n    X, y = validate_data(\n           ~~~~~~~~~~~~~^\n        self,\n        ^^^^^\n    ...<6 lines>...\n        y_numeric=True,\n        ^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/adhirajsen/Documents/Documents - Adhiraj’s MacBook Pro/UTA/Semesters/2. Fall2025/CSE6363/FinalProject/cse6363_finalProj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py\", line 2971, in validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/adhirajsen/Documents/Documents - Adhiraj’s MacBook Pro/UTA/Semesters/2. Fall2025/CSE6363/FinalProject/cse6363_finalProj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py\", line 1368, in check_X_y\n    X = check_array(\n        X,\n    ...<12 lines>...\n        input_name=\"X\",\n    )\n  File \"/Users/adhirajsen/Documents/Documents - Adhiraj’s MacBook Pro/UTA/Semesters/2. Fall2025/CSE6363/FinalProject/cse6363_finalProj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py\", line 1105, in check_array\n    _assert_all_finite(\n    ~~~~~~~~~~~~~~~~~~^\n        array,\n        ^^^^^^\n    ...<2 lines>...\n        allow_nan=ensure_all_finite == \"allow-nan\",\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/adhirajsen/Documents/Documents - Adhiraj’s MacBook Pro/UTA/Semesters/2. Fall2025/CSE6363/FinalProject/cse6363_finalProj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py\", line 120, in _assert_all_finite\n    _assert_all_finite_element_wise(\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        X,\n        ^^\n    ...<4 lines>...\n        input_name=input_name,\n        ^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/adhirajsen/Documents/Documents - Adhiraj’s MacBook Pro/UTA/Semesters/2. Fall2025/CSE6363/FinalProject/cse6363_finalProj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py\", line 169, in _assert_all_finite_element_wise\n    raise ValueError(msg_err)\nValueError: Input X contains NaN.\nRidge does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     55\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m df_clean[POSITION_COLUMN].isin([\u001b[33m'\u001b[39m\u001b[33mNaN\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mnan\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mNULL\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mnull\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mNone\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mN/A\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mn/a\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m]).any(), \\\n\u001b[32m     56\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPosition column still contains string missing values in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✓ All target and position columns are clean (no NaN, None, or string \u001b[39m\u001b[33m'\u001b[39m\u001b[33mNaN\u001b[39m\u001b[33m'\u001b[39m\u001b[33m values)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m baseline_result = \u001b[43mtrain_ridge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_df_clean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_df\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_df_clean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43malpha_grid\u001b[49m\u001b[43m=\u001b[49m\u001b[43mALPHA_GRID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_column\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTARGET_COLUMN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_column\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPOSITION_COLUMN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrop_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mIDENTIFIER_COLUMNS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscale_numeric\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBest alpha from RidgeCV: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbaseline_result[\u001b[33m'\u001b[39m\u001b[33malpha\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     71\u001b[39m \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m     72\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain RMSE=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbaseline_result[\u001b[33m'\u001b[39m\u001b[33mtrain_metrics\u001b[39m\u001b[33m'\u001b[39m].rmse\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     73\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mVal RMSE=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbaseline_result[\u001b[33m'\u001b[39m\u001b[33mval_metrics\u001b[39m\u001b[33m'\u001b[39m].rmse\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     74\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Documents - Adhiraj’s MacBook Pro/UTA/Semesters/2. Fall2025/CSE6363/FinalProject/cse6363_finalProj/notebooks/src/models.py:128\u001b[39m, in \u001b[36mtrain_ridge\u001b[39m\u001b[34m(train_df, val_df, alpha_grid, target_column, position_column, drop_columns, scale_numeric)\u001b[39m\n\u001b[32m    126\u001b[39m     tscv = TimeSeriesSplit(n_splits=n_splits)\n\u001b[32m    127\u001b[39m     model = RidgeCV(alphas=alpha_values, cv=tscv, scoring=\u001b[33m\"\u001b[39m\u001b[33mneg_mean_squared_error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m train_metrics: EvaluationResult = evaluate_split(model, X_train, y_train)\n\u001b[32m    131\u001b[39m val_metrics: EvaluationResult = evaluate_split(model, X_val, y_val)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Documents - Adhiraj’s MacBook Pro/UTA/Semesters/2. Fall2025/CSE6363/FinalProject/cse6363_finalProj/.venv/lib/python3.13/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Documents - Adhiraj’s MacBook Pro/UTA/Semesters/2. Fall2025/CSE6363/FinalProject/cse6363_finalProj/.venv/lib/python3.13/site-packages/sklearn/linear_model/_ridge.py:2694\u001b[39m, in \u001b[36mRidgeCV.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, **params)\u001b[39m\n\u001b[32m   2654\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   2655\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m, **params):\n\u001b[32m   2656\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Fit Ridge regression model with cv.\u001b[39;00m\n\u001b[32m   2657\u001b[39m \n\u001b[32m   2658\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2692\u001b[39m \u001b[33;03m    the validation score.\u001b[39;00m\n\u001b[32m   2693\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2694\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2695\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Documents - Adhiraj’s MacBook Pro/UTA/Semesters/2. Fall2025/CSE6363/FinalProject/cse6363_finalProj/.venv/lib/python3.13/site-packages/sklearn/linear_model/_ridge.py:2458\u001b[39m, in \u001b[36m_BaseRidgeCV.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, **params)\u001b[39m\n\u001b[32m   2449\u001b[39m     estimator.set_fit_request(sample_weight=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   2451\u001b[39m grid_search = GridSearchCV(\n\u001b[32m   2452\u001b[39m     estimator,\n\u001b[32m   2453\u001b[39m     parameters,\n\u001b[32m   2454\u001b[39m     cv=cv,\n\u001b[32m   2455\u001b[39m     scoring=scorer,\n\u001b[32m   2456\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2458\u001b[39m \u001b[43mgrid_search\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2459\u001b[39m estimator = grid_search.best_estimator_\n\u001b[32m   2460\u001b[39m \u001b[38;5;28mself\u001b[39m.alpha_ = grid_search.best_estimator_.alpha\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Documents - Adhiraj’s MacBook Pro/UTA/Semesters/2. Fall2025/CSE6363/FinalProject/cse6363_finalProj/.venv/lib/python3.13/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Documents - Adhiraj’s MacBook Pro/UTA/Semesters/2. Fall2025/CSE6363/FinalProject/cse6363_finalProj/.venv/lib/python3.13/site-packages/sklearn/model_selection/_search.py:1051\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1045\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1046\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1047\u001b[39m     )\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1051\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1055\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Documents - Adhiraj’s MacBook Pro/UTA/Semesters/2. Fall2025/CSE6363/FinalProject/cse6363_finalProj/.venv/lib/python3.13/site-packages/sklearn/model_selection/_search.py:1605\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1603\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1604\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1605\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Documents - Adhiraj’s MacBook Pro/UTA/Semesters/2. Fall2025/CSE6363/FinalProject/cse6363_finalProj/.venv/lib/python3.13/site-packages/sklearn/model_selection/_search.py:1028\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m   1021\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) != n_candidates * n_splits:\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1023\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcv.split and cv.get_n_splits returned \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1024\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1025\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(n_splits, \u001b[38;5;28mlen\u001b[39m(out) // n_candidates)\n\u001b[32m   1026\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1028\u001b[39m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[32m   1031\u001b[39m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[32m   1032\u001b[39m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[32m   1033\u001b[39m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[32m   1034\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.scoring):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Documents - Adhiraj’s MacBook Pro/UTA/Semesters/2. Fall2025/CSE6363/FinalProject/cse6363_finalProj/.venv/lib/python3.13/site-packages/sklearn/model_selection/_validation.py:505\u001b[39m, in \u001b[36m_warn_or_raise_about_fit_failures\u001b[39m\u001b[34m(results, error_score)\u001b[39m\n\u001b[32m    498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits == num_fits:\n\u001b[32m    499\u001b[39m     all_fits_failed_message = (\n\u001b[32m    500\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    501\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    502\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou can try to debug the error by setting error_score=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    503\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m505\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    508\u001b[39m     some_fits_failed_message = (\n\u001b[32m    509\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    510\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe score on these train-test partitions for these parameters\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    514\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    515\u001b[39m     )\n",
            "\u001b[31mValueError\u001b[39m: \nAll the 25 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n25 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/adhirajsen/Documents/Documents - Adhiraj’s MacBook Pro/UTA/Semesters/2. Fall2025/CSE6363/FinalProject/cse6363_finalProj/.venv/lib/python3.13/site-packages/sklearn/model_selection/_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/adhirajsen/Documents/Documents - Adhiraj’s MacBook Pro/UTA/Semesters/2. Fall2025/CSE6363/FinalProject/cse6363_finalProj/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 1365, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/Users/adhirajsen/Documents/Documents - Adhiraj’s MacBook Pro/UTA/Semesters/2. Fall2025/CSE6363/FinalProject/cse6363_finalProj/.venv/lib/python3.13/site-packages/sklearn/linear_model/_ridge.py\", line 1238, in fit\n    X, y = validate_data(\n           ~~~~~~~~~~~~~^\n        self,\n        ^^^^^\n    ...<6 lines>...\n        y_numeric=True,\n        ^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/adhirajsen/Documents/Documents - Adhiraj’s MacBook Pro/UTA/Semesters/2. Fall2025/CSE6363/FinalProject/cse6363_finalProj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py\", line 2971, in validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/adhirajsen/Documents/Documents - Adhiraj’s MacBook Pro/UTA/Semesters/2. Fall2025/CSE6363/FinalProject/cse6363_finalProj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py\", line 1368, in check_X_y\n    X = check_array(\n        X,\n    ...<12 lines>...\n        input_name=\"X\",\n    )\n  File \"/Users/adhirajsen/Documents/Documents - Adhiraj’s MacBook Pro/UTA/Semesters/2. Fall2025/CSE6363/FinalProject/cse6363_finalProj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py\", line 1105, in check_array\n    _assert_all_finite(\n    ~~~~~~~~~~~~~~~~~~^\n        array,\n        ^^^^^^\n    ...<2 lines>...\n        allow_nan=ensure_all_finite == \"allow-nan\",\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/adhirajsen/Documents/Documents - Adhiraj’s MacBook Pro/UTA/Semesters/2. Fall2025/CSE6363/FinalProject/cse6363_finalProj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py\", line 120, in _assert_all_finite\n    _assert_all_finite_element_wise(\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        X,\n        ^^\n    ...<4 lines>...\n        input_name=input_name,\n        ^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/adhirajsen/Documents/Documents - Adhiraj’s MacBook Pro/UTA/Semesters/2. Fall2025/CSE6363/FinalProject/cse6363_finalProj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py\", line 169, in _assert_all_finite_element_wise\n    raise ValueError(msg_err)\nValueError: Input X contains NaN.\nRidge does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n"
          ]
        }
      ],
      "source": [
        "ALPHA_GRID = [0.1, 0.3, 1.0, 3.0, 10.0]\n",
        "IDENTIFIER_COLUMNS = [\"player_id\", \"player_name\", \"team\"]\n",
        "TARGET_COLUMN = \"ppr_points\"\n",
        "POSITION_COLUMN = \"position\"\n",
        "\n",
        "# Clean target and position columns of any NaN/null/missing values before training\n",
        "print(\"Cleaning target and position columns...\")\n",
        "print(f\"Train: {len(train_df)} rows before cleaning\")\n",
        "print(f\"Val: {len(val_df)} rows before cleaning\")\n",
        "\n",
        "def clean_missing_values(df, columns):\n",
        "    \"\"\"Comprehensively clean missing values including NaN, None, and string representations.\"\"\"\n",
        "    df_clean = df.copy()\n",
        "    \n",
        "    for col in columns:\n",
        "        if col not in df_clean.columns:\n",
        "            continue\n",
        "            \n",
        "        # Replace string representations of missing values with actual NaN\n",
        "        # Handle common string representations: \"NaN\", \"nan\", \"NULL\", \"null\", \"\", etc.\n",
        "        if df_clean[col].dtype == 'object':\n",
        "            # For string/object columns (like position)\n",
        "            missing_strings = ['NaN', 'nan', 'NULL', 'null', 'None', 'N/A', 'n/a', '']\n",
        "            df_clean[col] = df_clean[col].replace(missing_strings, pd.NA)\n",
        "        else:\n",
        "            # For numeric columns (like target)\n",
        "            # Replace string \"NaN\" if somehow present in numeric column\n",
        "            if df_clean[col].dtype == 'object':\n",
        "                df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
        "            # Replace None values\n",
        "            df_clean[col] = df_clean[col].replace([None], pd.NA)\n",
        "    \n",
        "    # Drop rows with any missing values in the specified columns\n",
        "    df_clean = df_clean.dropna(subset=columns)\n",
        "    \n",
        "    return df_clean\n",
        "\n",
        "# Clean both dataframes\n",
        "train_df_clean = clean_missing_values(train_df, [TARGET_COLUMN, POSITION_COLUMN])\n",
        "val_df_clean = clean_missing_values(val_df, [TARGET_COLUMN, POSITION_COLUMN])\n",
        "\n",
        "print(f\"Train: {len(train_df_clean)} rows after cleaning (dropped {len(train_df) - len(train_df_clean)})\")\n",
        "print(f\"Val: {len(val_df_clean)} rows after cleaning (dropped {len(val_df) - len(val_df_clean)})\")\n",
        "\n",
        "# Verify no missing values remain (NaN, None, or string \"NaN\")\n",
        "for df_name, df_clean in [(\"train_df\", train_df_clean), (\"val_df\", val_df_clean)]:\n",
        "    # Check for NaN/NA values\n",
        "    assert df_clean[TARGET_COLUMN].notna().all(), f\"Target column still contains missing values in {df_name}\"\n",
        "    assert df_clean[POSITION_COLUMN].notna().all(), f\"Position column still contains missing values in {df_name}\"\n",
        "    # Check that no string \"NaN\" values remain (should be caught by notna(), but double-check)\n",
        "    if df_clean[TARGET_COLUMN].dtype == 'object':\n",
        "        assert not df_clean[TARGET_COLUMN].isin(['NaN', 'nan', 'NULL', 'null', 'None', 'N/A', 'n/a', '']).any(), \\\n",
        "            f\"Target column still contains string missing values in {df_name}\"\n",
        "    if df_clean[POSITION_COLUMN].dtype == 'object':\n",
        "        assert not df_clean[POSITION_COLUMN].isin(['NaN', 'nan', 'NULL', 'null', 'None', 'N/A', 'n/a', '']).any(), \\\n",
        "            f\"Position column still contains string missing values in {df_name}\"\n",
        "\n",
        "print(\"✓ All target and position columns are clean (no NaN, None, or string 'NaN' values)\")\n",
        "\n",
        "baseline_result = train_ridge(\n",
        "    train_df=train_df_clean,\n",
        "    val_df=val_df_clean,\n",
        "    alpha_grid=ALPHA_GRID,\n",
        "    target_column=TARGET_COLUMN,\n",
        "    position_column=POSITION_COLUMN,\n",
        "    drop_columns=IDENTIFIER_COLUMNS,\n",
        "    scale_numeric=True,\n",
        ")\n",
        "\n",
        "print(f\"Best alpha from RidgeCV: {baseline_result['alpha']:.3f}\")\n",
        "print(\n",
        "    f\"Train RMSE={baseline_result['train_metrics'].rmse:.2f}, \"\n",
        "    f\"Val RMSE={baseline_result['val_metrics'].rmse:.2f}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'baseline_result' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      8\u001b[39m rows = []\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m split_name, frame \u001b[38;5;129;01min\u001b[39;00m splits.items():\n\u001b[32m     10\u001b[39m     X_split, y_split, _ = build_feature_matrix(\n\u001b[32m     11\u001b[39m         frame.sort_values([\u001b[33m\"\u001b[39m\u001b[33mseason\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mplayer_id\u001b[39m\u001b[33m\"\u001b[39m]).reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m     12\u001b[39m         target_column=\u001b[33m\"\u001b[39m\u001b[33mppr_points\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     13\u001b[39m         position_column=\u001b[33m\"\u001b[39m\u001b[33mposition\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     14\u001b[39m         drop_columns=IDENTIFIER_COLUMNS,\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m         preprocessor=\u001b[43mbaseline_result\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mpreprocessor\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     16\u001b[39m         fit=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     17\u001b[39m     )\n\u001b[32m     18\u001b[39m     metrics = evaluate_split(baseline_result[\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], X_split, y_split)\n\u001b[32m     19\u001b[39m     rows.append(\n\u001b[32m     20\u001b[39m         {\n\u001b[32m     21\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mSplit\u001b[39m\u001b[33m\"\u001b[39m: split_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m         }\n\u001b[32m     26\u001b[39m     )\n",
            "\u001b[31mNameError\u001b[39m: name 'baseline_result' is not defined"
          ]
        }
      ],
      "source": [
        "splits = {\n",
        "    \"Train\": train_df,\n",
        "    \"2022 val\": val_df,\n",
        "    \"2023 test\": test_2023_df,\n",
        "    \"2024 test\": test_2024_df,\n",
        "}\n",
        "\n",
        "rows = []\n",
        "for split_name, frame in splits.items():\n",
        "    X_split, y_split, _ = build_feature_matrix(\n",
        "        frame.sort_values([\"season\", \"player_id\"]).reset_index(drop=True),\n",
        "        target_column=\"ppr_points\",\n",
        "        position_column=\"position\",\n",
        "        drop_columns=IDENTIFIER_COLUMNS,\n",
        "        preprocessor=baseline_result[\"preprocessor\"],\n",
        "        fit=False,\n",
        "    )\n",
        "    metrics = evaluate_split(baseline_result[\"model\"], X_split, y_split)\n",
        "    rows.append(\n",
        "        {\n",
        "            \"Split\": split_name,\n",
        "            \"MAE\": metrics.mae,\n",
        "            \"RMSE\": metrics.rmse,\n",
        "            \"R²\": metrics.r2,\n",
        "        }\n",
        "    )\n",
        "\n",
        "metrics_df = pd.DataFrame(rows).set_index(\"Split\")\n",
        "metrics_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Ridge baseline remains calibrated on older data but loses accuracy on recent seasons. The widening MAE/RMSE gaps between 2022 and 2024 quantify the concept-drift baseline we will try to close with adaptive models or transfer learning in later stages.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
